{"title":"Data strategies for future us","markdown":{"yaml":{"title":"Data strategies for future us"},"headingText":"Data organization in spreadsheets","containsRefs":false,"markdown":"\n\nData strategies are part of a shared workflow strategy: How do we structure data, where do we store and back up data, how do we create metadata, and keep the raw data raw...Here we will discuss personal and team habits for data management and sharing: data strategies for future us.\n\n**Slides** that have been presented during Champions Program Cohort Calls (with guest teachers indicated): \n\n- [Data strategies for Future Us](https://docs.google.com/presentation/d/1rv-JfJeuYhogxV6Dpn_hNDm09nfKnOMtmZpgcciI_98/edit?usp=sharing), also presented by [Ileana Fenwick](https://twitter.com/_ileanaf)\n  - [Data strategies; examples from NOAA Fisheries Alaska (Marine Mammal Lab Stock Assessment Report)](https://docs.google.com/presentation/d/1ed3tNPTM3hOBFEMJZKk9O06QFDltZOZoAdDEih48ufk/edit#slide=id.p), contributed by Rod Towell, Nancy Young, Tony Orr, Brian Fadely, Erin Richmond\n  - [Data stategies including data management plans (DMPs)](https://docs.google.com/presentation/d/145TOlLGH3qbJzPWdEJ2KS1LFdvP3MLTWgORHktb2R1k/edit#slide=id.p)\n- [Metadata](https://docs.google.com/presentation/d/10X5i9zZ0uVeEaTW6F2aSvkrcxLCgW0JU9rVZvPB1ZKc/edit#slide=id.g6532ff24f4_0_0), contributed by [Dr. Jessica Couture](https://www.jessicalcouture.com/)\n- [Data to Product Workflows](https://docs.google.com/presentation/d/12Jru3DReVH3sO-nG0msAjCNS8PVNuaYSxHi-Bxlkr1E/edit?usp=sharing), contributed by [Dr. Emily Markowitz](https://twitter.com/emilyhmarkowitz) ([video](https://youtu.be/S-w_74Pc908))\n\n---\n\n\nThis publication by [Broman & Woo, 2018](https://peerj.com/preprints/3183/) appears in the “Practical Data Science for Stats” collection in [PeerJ](https://peerj.com/collections/50-practicaldatascistats/) & [American Statistician](https://www.tandfonline.com/toc/utas20/72/1).  \n\nIt is a delightful read, from the first opening sentences:\n\n>“Spreadsheets, for all of their mundane rectangularness, have been the subject of angst and controversy for decades.… Amid this debate, spreadsheets have continued to play a significant role in researchers' workflows. The dangers of spreadsheets are real, however – so much so that the European Spreadsheet Risks Interest Group keeps a public archive of spreadsheet ‘horror stories’... ”\n\nBroman & Woo share practical tips to make spreadsheets less error-prone, easier for computers to process, and easier to share. And something incredibly cool, it's the 3rd most downloaded stats paper in the American Statistician, behind 2 papers about p-values ([twitter thread](https://twitter.com/kwbroman/status/1326678636649394176)).\n\nRead their paper for strategies behind their basic principles: \n\n1. Be consistent\n1. Write dates like YYYY-MM-DD\n1. Don't leave any cells empty\n1. Put just one thing in a cell\n1. Organize data as a rectangle (“Tidy data”)\n1. Create a data dictionary\n1. Don't include calculations in the raw data files\n1. Don't use font color or highlighting as data\n1. Choose good names for things\n1. Make backups\n1. Use data validation to avoid data entry errors\n1. Save the data in plain text files\n\n\n## Good enough practices in scientific computing. \n\nThis publication by [Wilson et al. 2017](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510) in PLoS Computational Biology follows a previous publication by [Wilson et al. 2014: Best practices for scientific computing](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745).\n\nIn terms of data management recommendataion, they have 2 main themes:\n\n1. work towards ready-to-analyze data incrementally, documenting both the intermediate data and the process\n1. embrace the idea of \"tidy data\", which can be a powerful accelerator for analysis\n\nRead their paper for strategies behind their basic principles (Box 1):\n\n1. Save the raw data.\n1. Ensure that raw data are backed up in more than one location.\n1. Create the data you wish to see in the world.\n1. Create analysis-friendly data.\n1. Record all the steps used to process data.\n1. Anticipate the need to use multiple tables, & use a unique identifier for every record.\n1. Submit data to a reputable DOI-issuing repository so that others can access & cite.\n\nThe publication also covers:\n\n- Software: write, organize, and share scripts and programs used in an analysis.\n- Collaboration: make it easy for existing and new collaborators to understand & contribute to a project.\n- Project organization: organize the digital artifacts of a project to ease discovery & understanding.\n- Tracking changes: record how various components of your project change over time.\n- Manuscripts: write manuscripts in a way that leaves an audit trail & minimizes manual merging of conflicts.\n\n<!---\nMake scientific data FAIR\n\nThis publication by [Stall et al 2019](https://www.nature.com/articles/d41586-019-01720-7) in *Nature*, says that all disciplines should follow the geosciences and demand best practice for publishing and sharing data. \n\nFAIR data means: \n\n- ‘Findable’ by anyone using common search tools\n- ‘Accessible’ so that the data and metadata can be examined\n- ‘Interoperable’ so that comparable data can be analysed and integrated through the use of common vocabulary and formats\n- ‘Reusable’ by others through robust metadata, provenance, usage licences\n\n> “Changes in geosciences policy and practice elevate data to valuable research contributions rather than files that are shoved in as an afterthought.” \n\nRead their paper for strategies and examples behind changing the culture: There are three big changes are crucial to shift research culture across all disciplines:\n\n1. Make depositing open and FAIR data a priority for all.\n1. Recognize and incentivize FAIR data practices.\n1. Fund global infrastructure to support FAIR data and tools.\n\n--->\n\n## Tidy data for efficiency, reproducibility, & collaboration\n\nBy [Lowndes & Horst 2020](https://www.openscapes.org/blog/2020/10/12/tidy-data/), posted on the Openscapes blog: an illustrated series to tell a story about tidy data.\n\nTidy data has been mentioned in each of the above, as a way to organize data in spreadsheets, to prepare ready-to-analyze data, and for sharing with the FAIR principles. \n\nSo what is tidy data? \n\nFirst, a preamble:\n\n- Raw data may not be stored in a tidy way\n- “Wrangling” data into tidy structure should be done programmatically as part of the analytical process - keep the raw data raw\n- Tidy data is a philosophy \n- There are existing tools to help\n\nRemember that \"tidying data (“data wrangling”) – up to 50–80% of a data scientist’s time\" [Lohr 2014, New York Times](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html), so it's important to leverage these existing philosophies and tools.\n\n\nWhen we talk about organizing data to help us work in an efficient, reproducible, and collaborative way, we are talking about TIDY DATA. We mean deliberately thinking about the shape and structure of data – something that might not seem super exciting but is truly game-changing. \n\nSo let’s talk about what tidy data is and why it is so empowering for your analytical life. \n\n### What is tidy data? \n\nTidy data is a way to describe data that’s organized with a particular structure – a rectangular structure, where each variable has its own column, and each observation has its own row (Wickham 2014). \n\n<center>\n  <img src=\"img/tidydata/tidydata_1.jpg\" width=\"500px\"></a>\n</center>\n<br>\n\nThis standard structure of tidy data led Hadley Wickham to describe it the way Leo Tolstoy describes families. Leo says “Happy families are all alike; every unhappy family is unhappy in its own way”. Similarly, Hadley says “tidy datasets are all alike, but every messy dataset is messy in its own way”.\n\n<center>\n  <img src=\"img/tidydata/tidydata_2.jpg\" width=\"500px\"></a>\n</center>\n<br>\n\n### Tidy data for more efficient data science\n\nTidy data allows you to be more efficient by using existing tools deliberately built to do the things you need to do, from subsetting portions of your data to plotting maps of your study area. Using existing tools saves you from building from scratch each time you work with a new dataset (which can be time-consuming and demoralizing). And luckily, there are a lot of tools specifically built to wrangle untidy data into tidy data (for example, in the tidyr package). By being more equipped to wrangle your data into a tidy format, you can get to your analyses faster to start answering the questions you're asking.\n\n<center>\n  <img src=\"img/tidydata/tidydata_3.jpg\" width=\"500px\"></a>\n</center>\n<br>\n\n### Tidy data for easier collaboration\n\nTidy data makes it easier to collaborate because our friends can use the same tools in a familiar way. Whether thinking about collaborators as current teammates, your future self, or future teammates, organizing and sharing data in a consistent and predictable way means less adjustment, time, and effort for all. \n<center>\n  <img src=\"img/tidydata/tidydata_4.jpg\" width=\"500px\"></a>\n</center>\n<br>\n\n### Tidy data for reproducibility and reuse\n\nTidy data also makes it easier to reproduce analyses because they are easier to understand, update, and reuse. By using tools together that all expect tidy data as inputs, you can build and iterate really powerful workflows. And, when you have additional data entries, it’s no problem to re-run your code!\n\n<center>\n  <img src=\"img/tidydata/tidydata_5.jpg\" width=\"500px\"></a>\n</center>\n<br>\n\n### Tidy data for the win! \n\nOnce you are empowered with tools to work with tidy data generally, it opens up a whole new world of datasets that feel more approachable because you can work using familiar tools. This transferrable confidence and ability to collaborate might be the best thing about tidy data.\n\n<center>\n  <img src=\"img/tidydata/tidydata_6.jpg\" width=\"500px\"></a>\n</center>\n<br>\n\nSo for more efficient, reproducible, and collaborative analyses, make friends with tidy data! \n\n<center>\n  <img src=\"img/tidydata/tidydata_7.jpg\" width=\"500px\"></a>\n</center>\n<br>\n\n-----\n\n## Learning Resources\n\n- Grolemund, G & Wickham, H (2016). R for Data Science: <https://r4ds.had.co.nz>\n  - See Ch 12: Tidy Data \n\n- Wickham, H (2014). _Tidy Data_. Journal of Statistical Software 58 (10). [jstatsoft.org/v59/i10/](http://www.jstatsoft.org/v59/i10/)\n\n- Broman, KW and KH Woo (2018). _Data Organization in Spreadsheets_. [The American Statistician 72 (1)](https://doi.org/10.1080/00031305.2017.1375989). Available open access as a [PeerJ preprint](https://peerj.com/preprints/3183/).\n\n- Leek, J (2016). [How to share data with a statistician](https://github.com/jtleek/datasharing)\n\n\n<!---\n\n\nWhat is metadata — Irene's/Jeannette's Amazon Dog Costume. \n\nDon't do a wholesale retrofit — but do better starting now. And when you come back to previous projects or need to hand a project off to someone (onboarding), spend a little time getting it a bit better set up as you go. \n\n\na good first step for many people is generating data dictionaries\nwhich are just regular (like tabular, csv) name and definitions of column names in various datasets they use\na good way to encourage people to use consistent naming schemes\nExample of a data dictionary: https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-14\n\n\nhttps://nceas.github.io/crescynt-training/\nhttps://nceas.github.io/crescynt-training/data-rescue.html\nhttps://nceas.github.io/crescynt-training/data-integration.html\n\nhttp://create.frictionlessdata.io/ — creates JSON, but lots of scientists use XML\n\nEML is a metadata standard (one of many) that uses XML to generate a regular structure\n\nhttp://training.arcticdata.io/materials/arctic-data-center-training/social-aspects-of-collaboration-and-data-policies.html\n\nHow to connect with databases from R: https://sciencificity-blog.netlify.app/posts/2020-12-12-using-the-tidyverse-with-databases/\n\n\n--->\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["include-files.lua","quarto"],"toc":true,"output-file":"data-strategies.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":{"light":["cosmo","theme.scss"],"dark":["cosmo","theme-dark.scss"]},"mainfont":"Atkinson Hyperlegible","code-copy":true,"title":"Data strategies for future us"},"extensions":{"book":{"multiFile":true}}}}}